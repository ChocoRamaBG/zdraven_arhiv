import time
import os
import urllib.parse
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# --- üìÅ –î–ò–ù–ê–ú–ò–ß–ï–ù –ü–™–¢ ---
# –í GitHub Actions —Ñ–∞–π–ª–æ–≤–µ—Ç–µ —Å–µ –∑–∞–ø–∏—Å–≤–∞—Ç –≤ —Ä–∞–±–æ—Ç–Ω–∞—Ç–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
script_dir = os.getcwd()
print(f"üìÇ –†–∞–±–æ—Ç–Ω–∞ –ø–∞–ø–∫–∞: {script_dir}")

output_filename = os.path.join(script_dir, "zdraven_arhiv_data.xlsx")
print(f"üéØ –ë–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏: {output_filename}")

# --- ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò –ù–ê –ë–†–ê–£–ó–™–†–ê –ó–ê CLOUD ---
options = Options()
# –í–ê–ñ–ù–û: –ó–∞–¥—ä–ª–∂–∏—Ç–µ–ª–Ω–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∑–∞ GitHub Actions (Headless Linux)
options.add_argument('--headless=new') 
options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--disable-gpu')
options.add_argument('--window-size=1920,1080')
# –°–ª–∞–≥–∞–º–µ User-Agent, –∑–∞ –¥–∞ –Ω–µ –Ω–∏ –º–∏—Å–ª—è—Ç –∑–∞ —Ä–æ–±–æ—Ç
options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

# --- üöó –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê –î–†–ê–ô–í–™–†–ß–û–í–¶–ò ---
print("‚è≥ –ü–∞–ª—è –≥—É–º–∏—Ç–µ –Ω–∞ Chrome (Headless Mode)...")
try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    print("‚úÖ –î—Ä–∞–π–≤—ä—Ä—ä—Ç –∑–∞—Ä–µ–¥–∏. Cloud Ninja Mode.")
except Exception as e:
    print(f"üí• –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ: {e}")
    raise e

# --- üíæ –ó–ê–ü–ò–°–í–ê–ß–ö–ê–¢–ê ---
def save_single_record(record):
    if not record: return
    try:
        new_df = pd.DataFrame([record])
        if os.path.exists(output_filename):
            try:
                existing_df = pd.read_excel(output_filename)
                final_df = pd.concat([existing_df, new_df], ignore_index=True)
            except:
                time.sleep(1)
                existing_df = pd.read_excel(output_filename)
                final_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            final_df = new_df

        final_df.to_excel(output_filename, index=False)
        print(f"üíæ Saved: {record.get('–ò–º–µ')}")
    except Exception as e:
        print(f"‚ùå ERROR saving: {e}")

# --- üïµÔ∏è‚Äç‚ôÇÔ∏è AGENT 007: PROFILE SCRAPER ---
def scrape_inner_profile(url, basic_info):
    print(f"   üëâ Visiting: {url}")
    try:
        driver.get(url)
        time.sleep(1) # –ü–æ-–º–∞–ª–∫–æ —á–∞–∫–∞–Ω–µ –∑–∞ cloud, —Ç–∞–º –Ω–µ—Ç—ä—Ç –µ –±—ä—Ä–∑
        
        try:
            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "elementor-heading-title")))
        except: pass

        # 1. –¢–ï–õ–ï–§–û–ù–ò
        phones = []
        try:
            phone_widgets = driver.find_elements(By.XPATH, "//div[contains(@class, 'elementor-widget-icon-box')]//i[contains(@class, 'fa-phone-alt')]/ancestor::div[contains(@class, 'elementor-widget-icon-box')]//h3")
            for pw in phone_widgets:
                t = pw.text.strip()
                if t and t not in phones: phones.append(t)
        except: pass
        if not phones: 
            try:
                links = driver.find_elements(By.XPATH, "//a[contains(@href, 'tel:')]")
                for l in links: phones.append(l.text.strip())
            except: pass

        # 2. –ò–ú–ï–ô–õ–ò
        emails = []
        try:
            email_widgets = driver.find_elements(By.XPATH, "//div[contains(@class, 'elementor-widget-icon-box')]//i[contains(@class, 'fa-mail-bulk')]/ancestor::div[contains(@class, 'elementor-widget-icon-box')]//h3")
            for ew in email_widgets:
                t = ew.text.strip()
                if t and t not in emails: emails.append(t)
        except: pass

        # 3. –ê–î–†–ï–°–ò & MAPS
        text_address = "-"
        try:
            addr_widgets = driver.find_elements(By.XPATH, "//div[contains(@class, 'elementor-widget-icon-box')]//i[contains(@class, 'icon-checked1')]/ancestor::div[contains(@class, 'elementor-widget-icon-box')]//h3")
            if addr_widgets:
                text_address = addr_widgets[0].text.strip()
        except: pass

        map_pin_address = "-"
        clickable_map_link = "-"
        try:
            iframe = driver.find_element(By.XPATH, "//div[contains(@class, 'elementor-widget-google_maps')]//iframe")
            raw_address = iframe.get_attribute("title") or iframe.get_attribute("aria-label")
            
            if raw_address:
                map_pin_address = raw_address
                encoded_address = urllib.parse.quote(raw_address)
                clickable_map_link = f"https://www.google.com/maps/search/?api=1&query={encoded_address}"
            else:
                src = iframe.get_attribute("src")
                if src: clickable_map_link = src
        except Exception: pass

        # 4. –ë–ò–û–ì–†–ê–§–ò–Ø
        full_bio = "-"
        try:
            bio_elements = driver.find_elements(By.CSS_SELECTOR, ".jet-listing-dynamic-field__content")
            bio_texts = []
            for el in bio_elements:
                txt = el.text.strip()
                if len(txt) > 40: 
                    bio_texts.append(txt)
            if bio_texts:
                full_bio = " || ".join(bio_texts).replace('\n', ' ')
            elif bio_elements:
                full_bio = bio_elements[0].text.strip().replace('\n', ' ')
        except: pass

        # 5. BREADCRUMB
        breadcrumb_info = "-"
        try:
            breadcrumb_el = driver.find_element(By.CLASS_NAME, "breadcrumb_last")
            breadcrumb_info = breadcrumb_el.text.strip()
        except: 
            try:
                breadcrumb_el = driver.find_element(By.CSS_SELECTOR, "#breadcrumbs span.breadcrumb_last")
                breadcrumb_info = breadcrumb_el.text.strip()
            except: pass

        basic_info.update({
            "–¢–µ–ª–µ—Ñ–æ–Ω–∏": ", ".join(phones) if phones else "-",
            "Email": ", ".join(emails) if emails else "-",
            "–ê–¥—Ä–µ—Å (–¢–µ–∫—Å—Ç)": text_address,
            "–ê–¥—Ä–µ—Å (Google Maps Pin)": map_pin_address,
            "Google Maps Link": clickable_map_link,
            "Breadcrumb (–¢–µ–∫—Å—Ç)": breadcrumb_info,
            "–ë–∏–æ–≥—Ä–∞—Ñ–∏—è": full_bio,
            "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        
    except Exception as e:
        print(f"üíÄ –ì—Ä–µ—à–∫–∞ –≤ –ø—Ä–æ—Ñ–∏–ª–∞: {e}")
        basic_info.update({"Note": "Profile Scrape Failed"})
    
    return basic_info

# --- üìú MAIN LOOP ---
page = 1
max_pages = 344 

print("üöÄ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Scraping –ø—Ä–æ—Ü–µ—Å–∞...")

try:
    while page <= max_pages:
        if page == 1:
            target_url = "https://zdraven-arhiv.com/doctors/"
        else:
            target_url = f"https://zdraven-arhiv.com/doctors/page/{page}/"
            
        print(f"\nüìÑ --- –°–¢–†–ê–ù–ò–¶–ê {page} –æ—Ç {max_pages} ---")
        driver.get(target_url)
        
        try:
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CLASS_NAME, "jet-listing-grid__item")))
            cards = driver.find_elements(By.XPATH, "//div[contains(@class, 'jet-listing-grid__item')]")
            
            if not cards:
                print("‚õî –ö—Ä–∞–π –Ω–∞ –º–∞—á–∞.")
                break

            print(f"üîé –ù–∞–º–µ—Ä–∏—Ö {len(cards)} –¥–æ–∫—Ç–æ—Ä–∏.")
            
            doctors_on_page = []
            for card in cards:
                try:
                    try:
                        link_el = card.find_element(By.XPATH, ".//a[contains(@class, 'jet-listing-dynamic-link__link')]")
                        url = link_el.get_attribute("href")
                        name = link_el.text.strip()
                        if not name:
                            name = card.find_element(By.XPATH, ".//span[contains(@class, 'jet-listing-dynamic-link__label')]").text.strip()
                    except: continue

                    desc_list = "-"
                    try:
                        fields = card.find_elements(By.XPATH, ".//div[contains(@class, 'jet-listing-dynamic-field__content')]")
                        if fields: desc_list = fields[0].text.strip()
                    except: pass

                    doc_data = {
                        "–ò–º–µ": name,
                        "URL": url,
                        "–û–ø–∏—Å–∞–Ω–∏–µ (–õ–∏—Å—Ç)": desc_list
                    }
                    doctors_on_page.append(doc_data)
                except: continue

            for doc in doctors_on_page:
                if not doc['URL']: continue
                full_data = scrape_inner_profile(doc['URL'], doc)
                save_single_record(full_data)

            page += 1
            
        except Exception as e:
            print(f"ü§¨ –ì–†–ï–®–ö–ê –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {e}")
            page += 1
            continue

finally:
    try:
        driver.quit()
        print("üõë –°–ø—Ä—è—Ö –∫–æ–ª–∞—Ç–∞.")
    except: pass
    print(f"\nüèÅ –§–∏–Ω–∏—Ç–æ!")
