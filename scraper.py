import time
import os
import urllib.parse
import pandas as pd
import re
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options

# --- üìÅ –î–ò–ù–ê–ú–ò–ß–ï–ù –ü–™–¢ ---
script_dir = os.getcwd()
print(f"üìÇ –†–∞–±–æ—Ç–Ω–∞ –ø–∞–ø–∫–∞: {script_dir}")

output_filename = os.path.join(script_dir, "zdraven_arhiv_data_fixed.xlsx")
print(f"üéØ –ë–∞–∑–∞—Ç–∞ –¥–∞–Ω–Ω–∏: {output_filename}")

# --- ‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò –ù–ê –ë–†–ê–£–ó–™–†–ê ---
options = Options()

# üëá –¢–û–í–ê –¢–†–Ø–ë–í–ê –î–ê –ï –í–ö–õ–Æ–ß–ï–ù–û, –©–û–ú –°–ò –ù–ê –°–™–†–í–™–†, –õ–¨–û–õ–¨–û!
options.add_argument('--headless=new') 

options.add_argument('--no-sandbox')
options.add_argument('--disable-dev-shm-usage')
options.add_argument('--disable-gpu')
options.add_argument('--window-size=1920,1080')
options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')

# --- üöó –°–¢–ê–†–¢–ò–†–ê–ù–ï –ù–ê –î–†–ê–ô–í–™–†–ß–û–í–¶–ò ---
print("‚è≥ –ü–∞–ª—è –≥—É–º–∏—Ç–µ –Ω–∞ Chrome... –∞–Ω–¥–∏–±—É–ª –º–æ—Ä–∫–æ–≤ mode activated.")
try:
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    print("‚úÖ –î—Ä–∞–π–≤—ä—Ä—ä—Ç –∑–∞—Ä–µ–¥–∏. –î–∞–≤–∞–π –¥–∞ –º–∞—á–∫–∞–º–µ.")
except Exception as e:
    print(f"üí• What the fuck? –ì—Ä–µ—à–∫–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ: {e}")
    raise e

# --- üíæ –ó–ê–ü–ò–°–í–ê–ß–ö–ê–¢–ê ---
def save_single_record(record):
    if not record: return
    try:
        new_df = pd.DataFrame([record])
        if os.path.exists(output_filename):
            try:
                existing_df = pd.read_excel(output_filename)
                final_df = pd.concat([existing_df, new_df], ignore_index=True)
            except:
                time.sleep(1)
                existing_df = pd.read_excel(output_filename)
                final_df = pd.concat([existing_df, new_df], ignore_index=True)
        else:
            final_df = new_df

        final_df.to_excel(output_filename, index=False)
        print(f"üíæ Saved: {record.get('–ò–º–µ')}")
    except Exception as e:
        print(f"‚ùå HELL ERROR saving: {e}")

# --- üïµÔ∏è‚Äç‚ôÇÔ∏è AGENT 007: PROFILE SCRAPER ---
def scrape_inner_profile(url, basic_info):
    print(f"   üëâ Visiting: {url}")
    try:
        driver.get(url)
        # –ß–∞–∫–∞–º–µ –º–∞–ª–∫–æ, –¥–∞ –Ω–µ –ø–æ–ª—É—á–∏–º 429 –∫–∞—Ç–æ –Ω—è–∫–æ–π –∞–º–∞—Ç—å–æ—Ä
        time.sleep(1.5) 
        
        # –ß–∞–∫–∞–º–µ –æ—Å–Ω–æ–≤–Ω–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä
        try:
            WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.CLASS_NAME, "elementor-widget-icon-box")))
        except: pass

        # --- –°–™–ë–ò–†–ê–ù–ï –ù–ê –í–°–ò–ß–ö–ò –ò–ö–û–ù-–ë–û–ö–°–ß–û–í–¶–ò ---
        # –í–º–µ—Å—Ç–æ –¥–∞ –≥–∞–¥–∞–µ–º –∏–∫–æ–Ω–∏—Ç–µ, –¥—ä—Ä–ø–∞–º–µ –≤—Å–∏—á–∫–∏ —Ç–µ–∫—Å—Ç–æ–≤–µ –æ—Ç –∫—É—Ç–∏–π–∫–∏—Ç–µ
        # –∏ –≥–∏ —Å–æ—Ä—Ç–∏—Ä–∞–º–µ —Å regex. –¢–æ–≤–∞ –µ *Gyatt level logic*.
        
        phones = []
        emails = []
        possible_addresses = []
        
        try:
            # –¢—ä—Ä—Å–∏–º –≤—Å–∏—á–∫–∏ –∑–∞–≥–ª–∞–≤–∏—è –≤ icon boxes
            box_titles = driver.find_elements(By.CSS_SELECTOR, ".elementor-widget-icon-box .elementor-icon-box-title span")
            
            for title_el in box_titles:
                text = title_el.text.strip()
                if not text: continue
                
                # Regex Logic - Brainrot style
                # –ê–∫–æ –∏–º–∞ @ - –∏–º–µ–π–ª
                if "@" in text:
                    if text not in emails: emails.append(text)
                # –ê–∫–æ –∏–º–∞ —Ü–∏—Ñ—Ä–∏ –∏ –µ —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª–Ω–æ –∫—Ä–∞—Ç–∫–æ - —Ç–µ–ª–µ—Ñ–æ–Ω
                elif re.search(r"(\+359|08[789]|02)", text) and len(text) < 20:
                    if text not in phones: phones.append(text)
                # –í—Å–∏—á–∫–æ –æ—Å—Ç–∞–Ω–∞–ª–æ, –∫–æ–µ—Ç–æ –µ –¥—ä–ª–≥–æ, –≤–µ—Ä–æ—è—Ç–Ω–æ –µ –∞–¥—Ä–µ—Å (–∏–ª–∏ –≥–ª—É–ø–æ—Å—Ç–∏)
                elif len(text) > 10:
                    if text not in possible_addresses: possible_addresses.append(text)
                    
        except Exception as e:
            print(f"‚ö†Ô∏è Warning: –ù–µ –º–æ–≥–∞ –¥–∞ –ø–∞—Ä—Å–Ω–∞ –±–æ–∫—Å—á–æ–≤—Ü–∏—Ç–µ. {e}")

        # --- –ê–î–†–ï–° –û–¢ GOOGLE MAPS IFRAME (–ù–∞–π-—Å–∏–≥—É—Ä–Ω–æ—Ç–æ, –ì–∞—â–Ω–∏–∫) ---
        map_pin_address = "-"
        clickable_map_link = "-"
        
        try:
            # –¢—ä—Ä—Å–∏–º iframe-–∞ –ø–æ –ø–æ-—É–º–µ–Ω –Ω–∞—á–∏–Ω
            iframe = driver.find_element(By.CSS_SELECTOR, "iframe[src*='maps.google.com']")
            raw_address = iframe.get_attribute("title") or iframe.get_attribute("aria-label")
            
            if raw_address:
                map_pin_address = raw_address
                encoded_address = urllib.parse.quote(raw_address)
                clickable_map_link = f"https://www.google.com/maps/search/?api=1&query={encoded_address}"
        except: 
            pass

        # –ê–∫–æ –Ω—è–º–∞–º–µ –∞–¥—Ä–µ—Å –æ—Ç –∫–∞—Ä—Ç–∏—Ç–µ, –≤–∑–∏–º–∞–º–µ –ø—ä—Ä–≤–∏—è –≤—ä–∑–º–æ–∂–µ–Ω —Ç–µ–∫—Å—Ç –æ—Ç –∫—É—Ç–∏–π–∫–∏—Ç–µ
        text_address = map_pin_address if map_pin_address != "-" else (possible_addresses[0] if possible_addresses else "-")

        # --- –ë–ò–û–ì–†–ê–§–ò–Ø ---
        full_bio = "-"
        try:
            # –í–∑–∏–º–∞–º–µ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –≥–ª–∞–≤–Ω–æ—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ
            bio_el = driver.find_element(By.XPATH, "//div[contains(@class, 'jet-listing-dynamic-field__content')]")
            full_bio = bio_el.get_attribute("innerText").strip().replace('\n', ' || ')
        except: pass

        # --- BREADCRUMB ---
        breadcrumb_info = "-"
        try:
            breadcrumb_el = driver.find_element(By.ID, "breadcrumbs")
            breadcrumb_info = breadcrumb_el.text.strip()
        except: pass

        basic_info.update({
            "–¢–µ–ª–µ—Ñ–æ–Ω–∏": ", ".join(phones) if phones else "-",
            "Email": ", ".join(emails) if emails else "-",
            "–ê–¥—Ä–µ—Å (–¢–µ–∫—Å—Ç)": text_address,
            "–ê–¥—Ä–µ—Å (Google Maps Pin)": map_pin_address,
            "Google Maps Link": clickable_map_link,
            "Breadcrumb (–¢–µ–∫—Å—Ç)": breadcrumb_info,
            "–ë–∏–æ–≥—Ä–∞—Ñ–∏—è": full_bio,
            "Timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        
    except Exception as e:
        print(f"üíÄ –ì—Ä–µ—à–∫–∞ –≤ –ø—Ä–æ—Ñ–∏–ª–∞: {e}. –ú–∞–º–∫–∞ –º—É —á–æ–≤–µ—á–µ!")
        basic_info.update({"Note": "Profile Scrape Failed"})
    
    return basic_info

# --- üìú MAIN LOOP (SIGMA GRINDSET EDITION) ---
page = 1
print("üöÄ –°—Ç–∞—Ä—Ç–∏—Ä–∞–Ω–µ –Ω–∞ Scraping –ø—Ä–æ—Ü–µ—Å–∞...")

try:
    while True:
        if page == 1:
            target_url = "https://zdraven-arhiv.com/doctors/"
        else:
            target_url = f"https://zdraven-arhiv.com/doctors/page/{page}/"
            
        print(f"\nüìÑ --- –°–¢–†–ê–ù–ò–¶–ê {page} ---")
        driver.get(target_url)
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞ 404 - –∞–∫–æ –Ω—è–º–∞ —Ç–∞–∫–∞–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞, –±–∏–µ–º —à—É—Ç–∞
            if "404" in driver.title or "–°—Ç—Ä–∞–Ω–∏—Ü–∞—Ç–∞ –Ω–µ –µ –Ω–∞–º–µ—Ä–µ–Ω–∞" in driver.page_source:
                 print("‚õî –£—Ü–µ–ª–∏—Ö–º–µ 404. –ö—Ä–∞–π –Ω–∞ –∏–≥—Ä–∞—Ç–∞, –ª—å–æ–ª—å–æ.")
                 break

            wait_time = 10 if page == 1 else 5
            try:
                WebDriverWait(driver, wait_time).until(EC.presence_of_element_located((By.CLASS_NAME, "jet-listing-grid__item")))
            except:
                print("‚õî –ù—è–º–∞ –µ–ª–µ–º–µ–Ω—Ç–∏. Probably finished.")
                break

            cards = driver.find_elements(By.XPATH, "//div[contains(@class, 'jet-listing-grid__item')]")
            if not cards: break

            print(f"üîé –ù–∞–º–µ—Ä–∏—Ö {len(cards)} –¥–æ–∫—Ç–æ—Ä–∏.")
            
            doctors_on_page = []
            for card in cards:
                try:
                    link_el = card.find_element(By.CSS_SELECTOR, "a.jet-listing-dynamic-link__link")
                    url = link_el.get_attribute("href")
                    name = link_el.text.strip()
                    
                    # –ú–∞–ª–∫–æ safe check
                    if not url: continue
                    
                    doc_data = {
                        "–ò–º–µ": name,
                        "URL": url,
                        "–û–ø–∏—Å–∞–Ω–∏–µ (–õ–∏—Å—Ç)": "-" # –ú—ä—Ä–∑–∏ –º–µ –¥–∞ –≥–æ –¥—ä—Ä–ø–∞–º –æ—Ç–≤—ä–Ω, —â–µ –≥–æ –≤–∑–µ–º–µ–º –æ—Ç–≤—ä—Ç—Ä–µ
                    }
                    doctors_on_page.append(doc_data)
                except: continue

            for doc in doctors_on_page:
                full_data = scrape_inner_profile(doc['URL'], doc)
                save_single_record(full_data)

            page += 1
            
        except Exception as e:
            print(f"ü§¨ –ì–†–ï–®–ö–ê –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ {page}: {e}")
            break

finally:
    try:
        driver.quit()
        print("üõë –°–ø—Ä—è—Ö –∫–æ–ª–∞—Ç–∞.")
    except: pass
    print(f"\nüèÅ –§–∏–Ω–∏—Ç–æ! –ê–Ω–¥–±–∏—É–ª –º–æ—Ä–∫–æ–≤ coding session finished.")
